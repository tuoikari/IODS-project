# 4: Clustering and classification

```{r}
date()
```

Load the data and explore it.
```{r}
# access the MASS package
library(MASS)

# load the data
data("Boston")

# explore the dataset
str(Boston)
summary(Boston)
```
The Boston data has 506 observations/rows and 14 columns/variables. It contains numerical and integer variables. It contains housing information in Boston area and has been collected by the U.S Census Service. The variables have differing scales which has to be taken into account when conducting analysis later.

The variables are the following:

    CRIM - per capita crime rate by town
    ZN - proportion of residential land zoned for lots over 25,000 sq.ft.
    INDUS - proportion of non-retail business acres per town.
    CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)
    NOX - nitric oxides concentration (parts per 10 million)
    RM - average number of rooms per dwelling
    AGE - proportion of owner-occupied units built prior to 1940
    DIS - weighted distances to five Boston employment centres
    RAD - index of accessibility to radial highways
    TAX - full-value property-tax rate per $10,000
    PTRATIO - pupil-teacher ratio by town
    B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
    LSTAT - % lower status of the population
    MEDV - Median value of owner-occupied homes in $1000's
    
Let's have look at the data with plots.

```{r}
pairs(Boston)
```
This plot is very small but you can get a feeling of variables in the data.
Let's print correlation matrix and visualize it by using corrplot to get a feeling of how different variables correlate with each other.

```{r}
library(tidyverse)

# calculate the correlation matrix and round it
cor_matrix <- cor(Boston) %>% round(digits = 2)

# print the correlation matrix
cor_matrix

# visualize the correlation matrix
library(corrplot)
corrplot(cor_matrix, method="circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)
```
Correlation plot shows correlation by the color and by the size of the circle.
The variable **crim** (per capita crime rate by town) was of interest in the exercise set. We can see that it has quite high (maybe something like 0.6, based on the colored plot) positive correlation with RAD (index of accessibility to radial highways and TAX (full-value property-tax rate per $10,000) variables.
Some variables have even higher correlations than crim. For example "indus" (proportion of non-retail business acres per town) is strongly positively correlated with "dis" (weighted distances to five Boston employment centres).

Here is the summary of the variables once more:
```{r}
summary(Boston)
```

As it can be seen, these variables are not standardized. They have quite differing scales. 

## Standardization

Thus, let's stardardize them. Standardization should work the following way:

"In the scaling we subtract the column means from the corresponding columns and divide the difference with standard deviation." (straight from the exercise set, wanted just include it here)

$$scaled(x) = \frac{x - mean(x)}{ sd(x)}$$
We can scale the variables by using scale() function.

```{r}
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)
```

The variables are all new centered around 0. The highest max-value is in "crim"-variable with max of 9.9.

To use this scaled version later, let's create this as a dataframe.

```{r}
boston_scaled <- as.data.frame(boston_scaled)
```

## Factor

Then let's turn "crim" variable to factor.

First, let's create quantile vector of crime.
```{r}
# summary of the scaled crime rate
summary(boston_scaled$crim)

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins
```

Let's then create crime vector based on bins created in previous chunk:
```{r}
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))
table(crime)
```

Now, let's add the new variable to the old data set.
```{r}
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```

Then, for clustering purposes, let's create train and test data sets. Let's take a random sample of rows in boston_scaled. Let's choose 80% of the rows to be used in training set. The rest will be used in the test set.  

```{r}
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)
```

## Lda

Then, let's use lda. We use the categorical crime rate as the target variable (lda takes as its first argument a formula with target variable and predictor variables) and all the other variables in the dataset as predictor variables.

I write some theory of LDA, mostly just for myself. The theory is from StatQuest video (https://www.youtube.com/watch?v=azXCzI57Yfc):

Lda creates a new axis tha maximizes the separtation of the categories. It maximizes the distance between the means of the categorie while minimizing the variation within each category. With multiple categories we first find a point central to all data and then measure distance between central point of each categories and central point of all data. Then we maximize the the distance between each category and the central point while minimizing the scatter for each category.

```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)
```

The plot shows some separation between the categories in the data based on crime variable. If I interpret this correctly, on first axis/component/LD1, the data can be separated to one certain cluster on the right which has big connection with "rad" variable and a rest of the cluster on the left. The data is then also separable futher on second axis, y-axis/LD2-axis which separates especially the cluster on the left to cluster with a lot of low-crime values and cluster of med_high crime values.

Let's then predict crime classes with LDA model. We can then compare the predictions to corrected results using correct_classes variable created earlier.

```{r}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```

High variable is predicted 100 correct. The model seemed to separate one group which had all the high cases so this follows what can be seen in the plot. Med_low class is predicted right 64% (16/25) of the cases. Med_high is predicted not so well as 13/21=61,9% of the cases are predicted right. Low class is worse as only 41,3% (12/29) of the cases is predicted correctly. These can be seen from the plot as a lot of the low-observations are blended with med_low and some of the with med_high observations.

## Distances

Then, let's calculate the distances between the observations. First, let's reload the data set.

```{r}
library(MASS)
data("Boston")
```

Let's scale the data set to get combarable distances.

```{r}
boston_scaled2 <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled2)
```

Let's then calculate distances. Let's use euclidean and manhattan distance measures and print summary of both of them for comparison.

```{r}
# euclidean distance matrix
dist_eu <- dist(boston_scaled2)

# look at the summary of the distances
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(boston_scaled2, method = "manhattan")

# look at the summary of the distances
summary(dist_man)
```

Distances might not tell much here. But counting distances is at the bottom of clustering, for example K-means clustering. Counting distances is part of analyzing the similarity of the data and different observations.

## K-means

```{r}
set.seed(13)

# k-means clustering
km <- kmeans(boston_scaled2, centers = 4)

# plot the Boston dataset with clusters
pairs(boston_scaled2, col = km$cluster)
```

```{r}
km
```

```{r}
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled2, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <- kmeans(Boston, centers = 10)

# plot the Boston dataset with clusters
pairs(boston_scaled2, col = km$cluster)
```
From the exercise set:
"When you plot the number of clusters and the total WCSS, the optimal number of clusters is when the total WCSS drops radically."

If I interpret the plot correctly, it would seem that with 2 cluster the twcss-value drops radically. Let's try with 2 clusters.

```{r}
km <- kmeans(boston_scaled2, centers = 2)

pairs(boston_scaled2, col = km$cluster)
```

I have hard time interpreting plots()-function output. But maybe this result makes the most sense as these plots are readable and I can see clear clusters with different variables. The plot with 4 center does not make so much sense as the cluster are not well separated, either with distance or by being thigh groups with similar observations in the same group.

It is also reasonable to have 2 clusters based on the results before. We got the same results of there being 2 clear cluster while using Lda-analysis. So, based on the plot showing twcss drop, based on the pairs() and based on comparing the clusters in k-means with Lda, 2 clusters seem to be the most reasonable grouping of this data.